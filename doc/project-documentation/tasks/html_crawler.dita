<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA Task//EN" "task.dtd">
<task id="html_crawler">
    <title>HTML Crawler</title>
    <shortdesc>Usually we don't use <b>HTML crawler</b> separately but in case you would like to,
        here are all the possible methods! We created our HTML crawler using <b>JSoup</b>
        library.</shortdesc>
    <taskbody>
        <context>
            <p>How to use Crawler and what's the goal of that class?</p>
        </context>
        <steps>
            <step>
                <cmd>First of all we have to create an instance of Crawler and send into constructor
                        <b>URL</b> to be crawled, <b>Base URL</b> and <b>isFile</b> argument. We use
                        <b>Base URL</b> in order to not get out of bounds. <b>isFile</b> is a
                    boolean that tells the Cralwer if we're working with files or websites, by
                    default it is set to false so you can skip it.</cmd>
                <info>
                    <codeblock>Crawler crawler = new Crawler(url, baseUrl, isFile); // Or
Crawler crawler = new Crawler(url, baseUrl); //By default "isFile" is false.</codeblock>
                </info>
            </step>
            <step>
                <cmd>To start the crawler you should call <b>crawl()</b> method, it doesn't receive
                    any arguments.</cmd>
                <info>
                    <codeblock>crawler.crawl();</codeblock>
                </info>
            </step>
            <step>
                <cmd>After that all we're all set. Now you could access the crawled data and use it
                    for your personal purposes. To get crawled pages we should call
                        <b>getCrawledPages()</b> method. It retuns a List of Pages. Page is a
                    created by us class that stores page's url, title, keywords and contents.</cmd>
                <info>
                    <codeblock>crawler.getCrawledPages();</codeblock>
                </info>
            </step>
            <step>
                <cmd>So the whole cycle would look like this:</cmd>
                <info>
                    <codeblock>Crawler crawler = new Crawler(url, baseUrl, isFile);
crawler.crawl();
List&lt;Page> pages = crawler.getCrawledPages();</codeblock>
                </info>
            </step>
            <step>
                <cmd>List of all the methods and explanations:</cmd>
                <info>
                    <codeblock>/**
* Constructor with url and baseUrl parameters.
* 
* @param url     is the page that should be crawled for data.
* @param baseUrl is the parent that is used to not go out of bounds.
* 
* @throws MalformedURLException if problems with initialization of URL
*                               occurred.
*/
public Crawler(final String url, final String baseUrl) throws MalformedURLException

/**
* Constructor with url, baseUrl and isFile parameters.
* 
* @param url     is the page that should be crawled for data.
* @param baseUrl is the parent that is used to not go out of bounds.
* @param isFile  is the state of URL
* 
* @throws MalformedURLException if problems with initialization of URL
*                               occurred.
*/
public Crawler(final String url, final String baseUrl, final boolean isFile) throws MalformedURLException

/**
* @return start url that should be crawled for data.
*/
public String getUrl()

/**
* @return base url that is used to not go out of parent's bounds.
*/
public String getBaseUrl()

/**
* @return true if URL is a file and false otherwise.
*/
public boolean isFile()

/**
* @param the value to set to "isFile" flag.
*/
public void setIsFile(final boolean isFile)

/**
* @return list of visited urls after the crawl.
*/
public List&lt;String> getVisitedUrls()

/**
* @return list of crawled pages
*/
public List&lt;Page> getCrawledPages()

/**
* Using the given url in the constructor it visits every resource that haves
* the same host and crawls its data.
* 
*/
public void crawl()</codeblock>
                </info>
            </step>
        </steps>
    </taskbody>
</task>
