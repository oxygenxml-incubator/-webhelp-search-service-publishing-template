package ro.sync.search;

import java.io.IOException;
import java.util.Arrays;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.algolia.search.SearchIndex;
import com.algolia.search.models.settings.IndexSettings;

/**
 * Class that uses DITA Conditional Profiling and pushes crawled data to Algolia
 * index.
 * 
 * @author Bozieac Artiom
 *
 */
public class AlgoliaFaceting extends AlgoliaBase {
	/**
	 * Logger to inform user about certain actions like errors and others.
	 */
	private static final Logger logger = LoggerFactory.getLogger(AlgoliaFaceting.class);
	/**
	 * Index that stores the current index performing actions on;
	 */
	protected SearchIndex<PageFaceting> index;

	/**
	 * Constructor with URL to get data from.
	 * 
	 * @throws IOException if a problem with loading config properties occured.
	 * 
	 */
	public AlgoliaFaceting() throws IOException {
		super();

	}

	/**
	 * Adds crawled pages from Crawler object to index.
	 * 
	 * @param url                     is the URL whose pages should be added to
	 *                                index.
	 * @param baseUrl                 is the base URL that is used to not go out of
	 *                                bounds.
	 * @param profilingConditionsPath is the path to the subject-scheme-values.json
	 *                                generated by Webhelp template.
	 * 
	 * @throws IOException if Crawler was failed to initiate or the HTML File
	 *                     couldn't be read.
	 */
	protected void populateIndex(final String url, final String baseUrl, final String profilingConditionsPath)
			throws IOException {
		CrawlerFaceting crawler = new CrawlerFaceting(url, baseUrl, false, profilingConditionsPath);
		crawler.crawl();

		index.setSettings(new IndexSettings()
				.setSearchableAttributes(Arrays.asList("title", "shortDescription", "content"))
				.setCustomRanking(Arrays.asList("desc(title)", "desc(shortDescription)", "desc(content)"))
				.setAttributesToHighlight(Arrays.asList("title", "shortDescription", "content"))
				.setAttributesToSnippet(Arrays.asList("content:30")).setAttributesForFaceting(
						Arrays.asList("_tags", "product", "platform", "audience", "rev", "props", "otherProps")));

		index.saveObjects(crawler.getCrawledPages());
		logger.info("{} Page object(s) successfully added to {} index!", crawler.getCrawledPages().size(),
				index.getUrlEncodedIndexName());
	}

	/**
	 * Use arguments to crawl the documentation and push it to Algolia index.
	 * 
	 * @param args is the array with indexName, url, baseUrl and
	 *             profilingConditionsPath.
	 * @throws IOException              if config.properties file is not set, path
	 *                                  to the documents is wrong or profilingPath
	 *                                  is invalid.
	 * @throws IllegalArgumentException if passed arguments are invalid.
	 */
	@Override
	public void useArguments(final String... args) throws IOException, IllegalArgumentException {
		String url = "";
		String baseUrl = "";
		String indexName = "";
		String profilingConditionsPath = "";

		for (String arg : args) {
			if (arg.startsWith("-url="))
				url = arg.substring(5, arg.length());
			else if (arg.startsWith("-baseUrl="))
				baseUrl = arg.substring(9, arg.length());
			else if (arg.startsWith("-indexName="))
				indexName = arg.substring(11, arg.length());
			else if (arg.startsWith("-profilingConditionsPath="))
				profilingConditionsPath = arg.substring(25, arg.length());
		}

		if (url.isEmpty() || baseUrl.isEmpty() || indexName.isEmpty() || profilingConditionsPath.isEmpty())
			throw new IllegalArgumentException();

		index = client.initIndex(indexName, PageFaceting.class);
		index.clearObjects();
		populateIndex(url, baseUrl, profilingConditionsPath);
	}
}
